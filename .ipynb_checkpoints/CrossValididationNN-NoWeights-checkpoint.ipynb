{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T06:34:41.511466Z",
     "start_time": "2019-03-13T06:34:35.938408Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from openpyxl import Workbook\n",
    "from openpyxl import load_workbook\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "import statsmodels.api as sm\n",
    "from sklearn import metrics\n",
    "from random import shuffle\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-13T06:45:57.554Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"data.csv\")\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "\n",
    "XLabels = ['TotalT','Temp','LSR','CA','Size', 'Moisture', 'IsoT', 'HeatT', 'Ramp','F_X', 'Ro', 'logRo', 'P']\n",
    "X = data[XLabels]\n",
    "#Scaling X\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "data[XLabels] = X\n",
    "\n",
    "papers = data['ID'].unique()\n",
    "shuffle(papers)\n",
    "\n",
    "#Remvoing papers from test list if they have less  than 10 points\n",
    "papersWithLessThanXPoints = []\n",
    "for paper in papers:\n",
    "    dataFromPaper = data[data['ID'] == paper]\n",
    "    if len(dataFromPaper.index) < 10:\n",
    "        papersWithLessThanXPoints.append(paper)\n",
    "papers = [x for x in papers if x not in papersWithLessThanXPoints]\n",
    "\n",
    "numPapers = len(papers)\n",
    "papersPerGroup = 2\n",
    "numBins = 10\n",
    "\n",
    "error_Frame = pd.DataFrame(columns=['ID', 'NN'])\n",
    "          \n",
    "numEpoch = 1500\n",
    "\n",
    "for paper in papers:\n",
    "    print(paper)\n",
    "    train_Frame = data[data['ID'] != paper]\n",
    "    test_Frame = data[data['ID'] == paper]\n",
    "    \n",
    "    papers = train_Frame['ID'].unique()\n",
    "    \n",
    "    \n",
    "    numPapers = len(papers)\n",
    "    #combos = [papers[x:x + papersPerGroup] for x in range(0, len(papers), papersPerGroup)]  \n",
    "    combos = []\n",
    "    for x in range(0, len(papers), papersPerGroup):\n",
    "        if x + papersPerGroup < len(papers):\n",
    "            combos.append(papers[x:x + papersPerGroup])\n",
    "        else:\n",
    "            combos.append(papers[x:])\n",
    "\n",
    "    lenTrain = len(train_Frame.index)\n",
    "    train_Frame, valid_Frame, train_valid_Frame = train_Frame.iloc[:int(lenTrain * 0.8), :], train_Frame.iloc[int(lenTrain * 0.8):, :], train_Frame\n",
    "\n",
    "    #Calculating Sample Weight\n",
    "    bins = train_Frame['Yield'].value_counts(bins=numBins)\n",
    "    for i in train_Frame.index:\n",
    "        for j in bins.index:\n",
    "            if int(train_Frame.at[i, 'Yield']) in j:\n",
    "                train_Frame.at[i, 'Sample_Weight'] = 100/bins[j].item()\n",
    "\n",
    "            \n",
    "    bins = train_valid_Frame['Yield'].value_counts(bins=numBins)\n",
    "\n",
    "    for i in train_valid_Frame.index:\n",
    "        for j in bins.index:\n",
    "            if int(train_valid_Frame.at[i, 'Yield']) in j:\n",
    "                train_valid_Frame.at[i, 'Sample_Weight'] = 100/bins[j].item()\n",
    "    \n",
    "    y_train, y_valid, y_test, y_train_valid = train_Frame['Yield'], valid_Frame['Yield'], test_Frame['Yield'], train_valid_Frame['Yield']\n",
    "    X_train, X_valid, X_test, X_train_valid = train_Frame[XLabels], valid_Frame[XLabels], test_Frame[XLabels], train_valid_Frame[XLabels]\n",
    "\n",
    "    train_weights = train_Frame['Sample_Weight']\n",
    "    train_valid_weights = train_valid_Frame['Sample_Weight']\n",
    "    \n",
    "\n",
    "    ##NN\n",
    "    learningRates = [0.002, 0.005, 0.01, 0.02]\n",
    "    batchSizes = [64, 128, 256, 512, 1024]\n",
    "    dropoutRates = [0.00, 0.001, 0.01, 0.1]\n",
    "    errors = []\n",
    "    for lr_ in learningRates:\n",
    "        for bs in batchSizes:\n",
    "            for dr in dropoutRates:\n",
    "                sumErrors = 0\n",
    "                for c in combos:\n",
    "#                     print(c)\n",
    "                    train_Frame = train_valid_Frame[~train_valid_Frame['ID'].isin(c)]\n",
    "                    valid_Frame = train_valid_Frame[train_valid_Frame['ID'].isin(c)]\n",
    "\n",
    "                    ##Calculating Sample Weight again just for train frame, because it's different\n",
    "                    bins = train_Frame['Yield'].value_counts(bins=numBins)\n",
    "                    for i in train_Frame.index:\n",
    "                        for j in bins.index:\n",
    "                            if int(train_Frame.at[i, 'Yield']) in j:\n",
    "                                train_Frame.at[i, 'Sample_Weight'] = 100/bins[j].item()\n",
    "\n",
    "                    y_train, y_valid, y_test, y_train_valid = train_Frame['Yield'], valid_Frame['Yield'], test_Frame['Yield'], train_valid_Frame['Yield']\n",
    "                    X_train, X_valid, X_test, X_train_valid = train_Frame[XLabels], valid_Frame[XLabels], test_Frame[XLabels], train_valid_Frame[XLabels]\n",
    "\n",
    "                    train_weights = train_Frame['Sample_Weight']\n",
    "                    \n",
    "#                     print(\"train\", train_Frame.isnull().any())\n",
    "#                     print(\"valid\", valid_Frame.isnull().any())\n",
    "#                     print(\"test\", test_Frame.isnull().any())\n",
    "                    \n",
    "                    model = Sequential()\n",
    "                    model.add(Dense(units=12, activation='sigmoid', input_dim=13))\n",
    "                    model.add(Dropout(dr))\n",
    "                    model.add(Dense(units=12, activation='sigmoid'))\n",
    "                    model.add(Dense(units=6, activation='sigmoid'))\n",
    "                    model.add(Dense(units=6, activation='sigmoid'))\n",
    "                    model.add(Dense(units=1, activation='softplus'))\n",
    "\n",
    "                    sgd = SGD(lr=lr_)  \n",
    "                    model.compile(loss='mean_squared_error', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "            \n",
    "#                     testNull = pd.isnull([X_train, y_train, X_valid])\n",
    "#                     print(testNull)\n",
    "                \n",
    "                    model.fit(X_train, y_train, epochs=numEpoch, batch_size=bs ,verbose=0)\n",
    "\n",
    "#                     loss_and_metrics = model.evaluate(X_valid, y_valid,batch_size=bs)\n",
    "#                     print(loss_and_metrics)\n",
    "                    \n",
    "#                     print(X_valid)\n",
    "                    y_pred = model.predict(X_valid, batch_size=bs)\n",
    "                    y_pred = y_pred.flatten()\n",
    "                    error =  metrics.mean_absolute_error(y_valid, y_pred)\n",
    "                    sumErrors = sumErrors + error\n",
    "                errors.append(sumErrors)\n",
    "    index_of_lowest_error = np.argmin(errors)\n",
    "          \n",
    "    print(\"Lowest Error In Validation _MSE_ \", np.min(errors))\n",
    "\n",
    "    best_lr = learningRates[int(index_of_lowest_error / (len(batchSizes) * len(dropoutRates)))] #Good\n",
    "    best_bs = batchSizes[int((index_of_lowest_error % (len(batchSizes) * len(dropoutRates)))/len(dropoutRates))] #Good\n",
    "    best_dr = dropoutRates[index_of_lowest_error % len(dropoutRates)] #Good \n",
    "    print(\"Best Learning Rate is: \",best_lr)\n",
    "    print(\"Best Batch Size is: \", best_bs)\n",
    "    print(\"Best Dropout Rate is: \", best_dr)    \n",
    "\n",
    "\n",
    "    #Using best values\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=12, activation='sigmoid', input_dim=13))\n",
    "    model.add(Dropout(best_dr))\n",
    "    model.add(Dense(units=12, activation='sigmoid'))\n",
    "    model.add(Dense(units=6, activation='sigmoid'))\n",
    "    model.add(Dense(units=6, activation='sigmoid'))\n",
    "    model.add(Dense(units=1, activation='softplus'))\n",
    "\n",
    "    sgd = SGD(lr=best_lr)  \n",
    "    model.compile(loss='mean_squared_error', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_train_valid, y_train_valid, epochs=numEpoch, batch_size=best_bs ,verbose=0)\n",
    "\n",
    "    loss_and_metrics = model.evaluate(X_test, y_test,batch_size=best_bs)\n",
    "\n",
    "    y_pred = model.predict(X_test, batch_size=bs)\n",
    "    y_pred = y_pred.flatten()\n",
    "          \n",
    "    mseNN = metrics.mean_absolute_error(y_test, y_pred)\n",
    "  \n",
    "\n",
    "    row = [[paper, mseNN]]\n",
    "    tempDf = pd.DataFrame(row, columns=['ID', 'NN'])\n",
    "    error_Frame = pd.concat([error_Frame, tempDf], ignore_index=True)\n",
    "    error_Frame.index = error_Frame['ID'].values\n",
    "    error_Frame = error_Frame.sort_index()\n",
    "    error_Frame.to_csv(\"CrossValidNNoWeights.csv\")\n",
    "\n",
    "\n",
    "    \n",
    "error_Frame.reset_index()\n",
    "error_Frame.index = error_Frame['ID'].values\n",
    "error_Frame = error_Frame.sort_index()\n",
    "\n",
    "\n",
    "error_Frame.plot(kind='bar', stacked=False)\n",
    "error_Frame.to_csv(\"CrossValidNNoWeights.csv\")\n",
    "\n",
    "plt.ylabel('Mean Absolute Error (Yield %)')\n",
    "plt.title(\"Accuracy of Models\")\n",
    "plt.savefig(\"CrossValidNNNoWeight.png\", dpi=600, bbox_inches='tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
