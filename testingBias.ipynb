{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-09T03:51:30.070275Z",
     "start_time": "2019-02-09T03:51:29.316755Z"
    }
   },
   "outputs": [],
   "source": [
    "from openpyxl import Workbook\n",
    "from openpyxl import load_workbook\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-09T03:51:31.199025Z",
     "start_time": "2019-02-09T03:51:30.071906Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         TotalT    Temp    LSR        CA   Size           Mass  Moisture  \\\n",
      "0    532.192261  373.65   8.00  0.407872  8.000      16.670000      20.0   \n",
      "1    110.535610  373.65   8.00  0.203936  8.000      16.670000      20.0   \n",
      "2     20.000000  403.15   8.00  0.810463  0.500       3.000000       0.0   \n",
      "3    113.347790  408.15   8.00  0.203936  8.000      16.670000      20.0   \n",
      "4    116.306262  398.15   8.00  0.101968  8.000      16.670000      20.0   \n",
      "5    201.588715  373.65   8.00  0.203936  8.000      16.670000      20.0   \n",
      "6     53.552343  373.65   8.00  0.407872  8.000      16.670000      20.0   \n",
      "7    155.661484  398.15   8.00  0.101968  8.000      16.670000      20.0   \n",
      "8    300.000000  423.15   3.70  0.000000  2.000      21.276596      10.0   \n",
      "9     76.333330  408.15   8.00  0.101968  8.000      16.670000      20.0   \n",
      "10     0.000000  398.15   8.00  0.405232  0.500       3.000000       0.0   \n",
      "11   151.687725  373.65   8.00  0.203936  8.000      16.670000      20.0   \n",
      "12    15.221088  398.15   8.00  0.407872  8.000      16.670000      20.0   \n",
      "13   315.000000  433.15   4.76  0.000000  5.000  263200.000000      39.0   \n",
      "14    38.571429  463.15  15.00  0.000000  0.325      37.500000       0.0   \n",
      "15    24.082213  398.15   8.00  0.305904  8.000      16.670000      20.0   \n",
      "16   530.897304  373.65   8.00  0.305904  8.000      16.670000      20.0   \n",
      "17   443.266325  373.65   8.00  0.305904  8.000      16.670000      20.0   \n",
      "18   212.876070  388.15   8.00  0.203936  8.000      16.670000      20.0   \n",
      "19    95.243665  388.15   8.00  0.203936  8.000      16.670000      20.0   \n",
      "20    41.192676  373.65   8.00  0.203936  8.000      16.670000      20.0   \n",
      "21   242.416798  373.65   8.00  0.203936  8.000      16.670000      20.0   \n",
      "22    85.833294  408.15   8.00  0.407872  8.000      16.670000      20.0   \n",
      "23    30.486976  408.15   8.00  0.101968  8.000      16.670000      20.0   \n",
      "24    74.733242  408.15   8.00  0.305904  8.000      16.670000      20.0   \n",
      "25   128.533408  408.15   8.00  0.203936  8.000      16.670000      20.0   \n",
      "26    60.000000  398.15   8.00  0.810463  0.500       3.000000       0.0   \n",
      "27   334.093214  373.65   8.00  0.407872  8.000      16.670000      20.0   \n",
      "28   110.306828  408.15   8.00  0.101968  8.000      16.670000      20.0   \n",
      "29    40.000000  396.15   8.00  1.013079  0.500       3.000000       0.0   \n",
      "..          ...     ...    ...       ...    ...            ...       ...   \n",
      "514   81.093750  433.15   6.00  0.000000  5.000     671.428571      10.5   \n",
      "515  147.901361  398.15   8.00  0.407872  8.000      16.670000      20.0   \n",
      "516  203.005984  388.15   8.00  0.305904  8.000      16.670000      20.0   \n",
      "517  248.467628  373.65   8.00  0.305904  8.000      16.670000      20.0   \n",
      "518   68.958996  398.15   8.00  0.101968  8.000      16.670000      20.0   \n",
      "519  164.093761  373.65   8.00  0.407872  8.000      16.670000      20.0   \n",
      "520   34.033406  398.15   8.00  0.305904  8.000      16.670000      20.0   \n",
      "521   34.295239  408.15   8.00  0.101968  8.000      16.670000      20.0   \n",
      "522   34.747585  388.15   8.00  0.203936  8.000      16.670000      20.0   \n",
      "523   35.922038  398.15   8.00  0.203936  8.000      16.670000      20.0   \n",
      "524   16.527402  388.15   8.00  0.407872  8.000      16.670000      20.0   \n",
      "525  200.714286  443.15  15.00  0.000000  0.590      37.500000       0.0   \n",
      "526   70.000000  403.15   8.00  0.405232  0.500       3.000000       0.0   \n",
      "527   52.955166  388.15   8.00  0.203936  8.000      16.670000      20.0   \n",
      "528  192.191775  373.65   8.00  0.203936  8.000      16.670000      20.0   \n",
      "529   42.857143  493.15  15.00  0.000000  0.325      37.500000       0.0   \n",
      "530   53.950502  388.15   8.00  0.305904  8.000      16.670000      20.0   \n",
      "531   51.402453  398.15   8.00  0.203936  8.000      16.670000      20.0   \n",
      "532  149.049112  388.15   8.00  0.101968  8.000      16.670000      20.0   \n",
      "533  232.350530  373.65   8.00  0.407872  8.000      16.670000      20.0   \n",
      "534  100.000000  403.15   8.00  0.405232  0.500       3.000000       0.0   \n",
      "535   61.976029  398.15   8.00  0.101968  8.000      16.670000      20.0   \n",
      "536   60.000000  398.15   8.00  1.215695  0.500       3.000000       0.0   \n",
      "537   63.853905  408.15   8.00  0.203936  8.000      16.670000      20.0   \n",
      "538   60.000000  398.15   8.00  0.405232  0.500       3.000000       0.0   \n",
      "539   50.235892  388.15   8.00  0.101968  8.000      16.670000      20.0   \n",
      "540  192.746126  388.15   8.00  0.101968  8.000      16.670000      20.0   \n",
      "541  210.985655  373.65   8.00  0.203936  8.000      16.670000      20.0   \n",
      "542   80.714286  443.15  15.00  0.000000  0.325      37.500000       0.0   \n",
      "543   34.790186  408.15   8.00  0.305904  8.000      16.670000      20.0   \n",
      "\n",
      "           IsoT       HeatT        Ramp    ...             M         R  \\\n",
      "0    522.754761    9.437500    8.000000    ...           NaN       NaN   \n",
      "1    101.098110    9.437500    8.000000    ...           NaN       NaN   \n",
      "2     18.950000    1.050000  100.000000    ...           NaN       NaN   \n",
      "3     99.597790   13.750000    8.000000    ...           NaN       NaN   \n",
      "4    103.806262   12.500000    8.000000    ...           NaN       NaN   \n",
      "5    192.151215    9.437500    8.000000    ...           NaN       NaN   \n",
      "6     44.114843    9.437500    8.000000    ...           NaN       NaN   \n",
      "7    143.161484   12.500000    8.000000    ...           NaN       NaN   \n",
      "8    292.236025    7.763975   16.100000    ...      0.232432       NaN   \n",
      "9     62.583330   13.750000    8.000000    ...           NaN       NaN   \n",
      "10     0.000000    0.000000  103.201147    ...           NaN       NaN   \n",
      "11   142.250225    9.437500    8.000000    ...           NaN       NaN   \n",
      "12     2.721088   12.500000    8.000000    ...           NaN       NaN   \n",
      "13   137.812500  177.187500    0.761905    ...      0.018016  0.000000   \n",
      "14    15.000000   23.571429    7.000000    ...           NaN       NaN   \n",
      "15    11.582213   12.500000    8.000000    ...           NaN       NaN   \n",
      "16   521.459804    9.437500    8.000000    ...           NaN       NaN   \n",
      "17   433.828825    9.437500    8.000000    ...           NaN       NaN   \n",
      "18   201.626070   11.250000    8.000000    ...           NaN       NaN   \n",
      "19    83.993665   11.250000    8.000000    ...           NaN       NaN   \n",
      "20    31.755176    9.437500    8.000000    ...           NaN       NaN   \n",
      "21   232.979298    9.437500    8.000000    ...           NaN       NaN   \n",
      "22    72.083294   13.750000    8.000000    ...           NaN       NaN   \n",
      "23    16.736976   13.750000    8.000000    ...           NaN       NaN   \n",
      "24    60.983242   13.750000    8.000000    ...           NaN       NaN   \n",
      "25   114.783408   13.750000    8.000000    ...           NaN       NaN   \n",
      "26    59.031019    0.968981  103.201147    ...           NaN       NaN   \n",
      "27   324.655714    9.437500    8.000000    ...           NaN       NaN   \n",
      "28    96.556828   13.750000    8.000000    ...           NaN       NaN   \n",
      "29    39.050398    0.949602  103.201147    ...           NaN       NaN   \n",
      "..          ...         ...         ...    ...           ...       ...   \n",
      "514   60.000000   21.093750    6.400000    ...      1.366667  0.583333   \n",
      "515  135.401361   12.500000    8.000000    ...           NaN       NaN   \n",
      "516  191.755984   11.250000    8.000000    ...           NaN       NaN   \n",
      "517  239.030128    9.437500    8.000000    ...           NaN       NaN   \n",
      "518   56.458996   12.500000    8.000000    ...           NaN       NaN   \n",
      "519  154.656261    9.437500    8.000000    ...           NaN       NaN   \n",
      "520   21.533406   12.500000    8.000000    ...           NaN       NaN   \n",
      "521   20.545239   13.750000    8.000000    ...           NaN       NaN   \n",
      "522   23.497585   11.250000    8.000000    ...           NaN       NaN   \n",
      "523   23.422038   12.500000    8.000000    ...           NaN       NaN   \n",
      "524    5.277402   11.250000    8.000000    ...           NaN       NaN   \n",
      "525  180.000000   20.714286    7.000000    ...           NaN       NaN   \n",
      "526   68.982569    1.017431  103.201147    ...           NaN       NaN   \n",
      "527   41.705166   11.250000    8.000000    ...           NaN       NaN   \n",
      "528  182.754275    9.437500    8.000000    ...           NaN       NaN   \n",
      "529   15.000000   27.857143    7.000000    ...           NaN       NaN   \n",
      "530   42.700502   11.250000    8.000000    ...           NaN       NaN   \n",
      "531   38.902453   12.500000    8.000000    ...           NaN       NaN   \n",
      "532  137.799112   11.250000    8.000000    ...           NaN       NaN   \n",
      "533  222.913030    9.437500    8.000000    ...           NaN       NaN   \n",
      "534   98.950000    1.050000  100.000000    ...           NaN       NaN   \n",
      "535   49.476029   12.500000    8.000000    ...           NaN       NaN   \n",
      "536   59.031019    0.968981  103.201147    ...           NaN       NaN   \n",
      "537   50.103905   13.750000    8.000000    ...           NaN       NaN   \n",
      "538   59.031019    0.968981  103.201147    ...           NaN       NaN   \n",
      "539   38.985892   11.250000    8.000000    ...           NaN       NaN   \n",
      "540  181.496126   11.250000    8.000000    ...           NaN       NaN   \n",
      "541  201.548155    9.437500    8.000000    ...           NaN       NaN   \n",
      "542   60.000000   20.714286    7.000000    ...      0.180000       NaN   \n",
      "543   21.040186   13.750000    8.000000    ...           NaN       NaN   \n",
      "\n",
      "         Furf  HMF                                             Source  \\\n",
      "0         NaN  NaN  Generation of xylose solutions from Eucalyptus...   \n",
      "1         NaN  NaN  Generation of xylose solutions from Eucalyptus...   \n",
      "2         NaN  NaN    KineticStuidesOfAcidHyrolysisOfMerantiWood.xlsx   \n",
      "3         NaN  NaN  Generation of xylose solutions from Eucalyptus...   \n",
      "4         NaN  NaN  Generation of xylose solutions from Eucalyptus...   \n",
      "5         NaN  NaN  Generation of xylose solutions from Eucalyptus...   \n",
      "6         NaN  NaN  Generation of xylose solutions from Eucalyptus...   \n",
      "7         NaN  NaN  Generation of xylose solutions from Eucalyptus...   \n",
      "8    0.113514  NaN  HemicelluloseExtractionOfMixedSouthernHardwood...   \n",
      "9         NaN  NaN  Generation of xylose solutions from Eucalyptus...   \n",
      "10        NaN  NaN             ProductionOfXyloseFromMerantiWood.xlsx   \n",
      "11        NaN  NaN  Generation of xylose solutions from Eucalyptus...   \n",
      "12        NaN  NaN  Generation of xylose solutions from Eucalyptus...   \n",
      "13   0.037833  NaN                          HotWaterPretreatment.xlsx   \n",
      "14        NaN  NaN             OptimizationOfHydrothermalCHEMPUP.xlsx   \n",
      "15        NaN  NaN  Generation of xylose solutions from Eucalyptus...   \n",
      "16        NaN  NaN  Generation of xylose solutions from Eucalyptus...   \n",
      "17        NaN  NaN  Generation of xylose solutions from Eucalyptus...   \n",
      "18        NaN  NaN  Generation of xylose solutions from Eucalyptus...   \n",
      "19        NaN  NaN  Generation of xylose solutions from Eucalyptus...   \n",
      "20        NaN  NaN  Generation of xylose solutions from Eucalyptus...   \n",
      "21        NaN  NaN  Generation of xylose solutions from Eucalyptus...   \n",
      "22        NaN  NaN  Generation of xylose solutions from Eucalyptus...   \n",
      "23        NaN  NaN  Generation of xylose solutions from Eucalyptus...   \n",
      "24        NaN  NaN  Generation of xylose solutions from Eucalyptus...   \n",
      "25        NaN  NaN  Generation of xylose solutions from Eucalyptus...   \n",
      "26        NaN  NaN             ProductionOfXyloseFromMerantiWood.xlsx   \n",
      "27        NaN  NaN  Generation of xylose solutions from Eucalyptus...   \n",
      "28        NaN  NaN  Generation of xylose solutions from Eucalyptus...   \n",
      "29        NaN  NaN             ProductionOfXyloseFromMerantiWood.xlsx   \n",
      "..        ...  ...                                                ...   \n",
      "514  0.183333  0.0  QuantitativeAnalysisofSugarsInWoodHydrolyzates...   \n",
      "515       NaN  NaN  Generation of xylose solutions from Eucalyptus...   \n",
      "516       NaN  NaN  Generation of xylose solutions from Eucalyptus...   \n",
      "517       NaN  NaN  Generation of xylose solutions from Eucalyptus...   \n",
      "518       NaN  NaN  Generation of xylose solutions from Eucalyptus...   \n",
      "519       NaN  NaN  Generation of xylose solutions from Eucalyptus...   \n",
      "520       NaN  NaN  Generation of xylose solutions from Eucalyptus...   \n",
      "521       NaN  NaN  Generation of xylose solutions from Eucalyptus...   \n",
      "522       NaN  NaN  Generation of xylose solutions from Eucalyptus...   \n",
      "523       NaN  NaN  Generation of xylose solutions from Eucalyptus...   \n",
      "524       NaN  NaN  Generation of xylose solutions from Eucalyptus...   \n",
      "525       NaN  NaN                 OptimizationofHydrothermalACS.xlsx   \n",
      "526       NaN  NaN  DesignOfProcessParametersForProudctionOfXylose...   \n",
      "527       NaN  NaN  Generation of xylose solutions from Eucalyptus...   \n",
      "528       NaN  NaN  Generation of xylose solutions from Eucalyptus...   \n",
      "529       NaN  NaN             OptimizationOfHydrothermalCHEMPUP.xlsx   \n",
      "530       NaN  NaN  Generation of xylose solutions from Eucalyptus...   \n",
      "531       NaN  NaN  Generation of xylose solutions from Eucalyptus...   \n",
      "532       NaN  NaN  Generation of xylose solutions from Eucalyptus...   \n",
      "533       NaN  NaN  Generation of xylose solutions from Eucalyptus...   \n",
      "534       NaN  NaN    KineticStuidesOfAcidHyrolysisOfMerantiWood.xlsx   \n",
      "535       NaN  NaN  Generation of xylose solutions from Eucalyptus...   \n",
      "536       NaN  NaN             ProductionOfXyloseFromMerantiWood.xlsx   \n",
      "537       NaN  NaN  Generation of xylose solutions from Eucalyptus...   \n",
      "538       NaN  NaN             ProductionOfXyloseFromMerantiWood.xlsx   \n",
      "539       NaN  NaN  Generation of xylose solutions from Eucalyptus...   \n",
      "540       NaN  NaN  Generation of xylose solutions from Eucalyptus...   \n",
      "541       NaN  NaN  Generation of xylose solutions from Eucalyptus...   \n",
      "542       NaN  NaN             OptimizationOfHydrothermalCHEMPUP.xlsx   \n",
      "543       NaN  NaN  Generation of xylose solutions from Eucalyptus...   \n",
      "\n",
      "         Yield            P      logP            Ro      logRo  \n",
      "0    79.673635     9.175709  2.216560    540.779032   6.293011  \n",
      "1    60.829590     1.774535  0.573539    104.583912   4.649990  \n",
      "2    42.162902     6.407922  1.857535    144.850509   4.975702  \n",
      "3    69.567001    53.297667  3.975893   1068.510963   6.974021  \n",
      "4    79.007148    21.926743  3.087707    565.348046   6.337442  \n",
      "5    76.305694     3.372755  1.215730    198.776474   5.292181  \n",
      "6    62.524216     0.774331 -0.255756     45.635896   3.820695  \n",
      "7    77.731315    30.239650  3.409154    779.683840   6.658889  \n",
      "8    10.697674   580.733806  6.364292   8667.990689   9.067392  \n",
      "9    78.064512    33.490156  3.511252    671.410221   6.509380  \n",
      "10    0.000000     0.000000  0.000000      0.000000   0.000000  \n",
      "11   70.015544     2.496862  0.915035    147.154927   4.991486  \n",
      "12   51.423471     0.574769 -0.553787     14.819549   2.695947  \n",
      "13   22.342181   624.402255  6.436795   8052.119888   8.993691  \n",
      "14    4.251969   650.617828  6.477922   6699.211530   8.809745  \n",
      "15   70.870701     2.446482  0.894651     63.078865   4.144386  \n",
      "16   79.447544     9.152979  2.214079    539.439425   6.290530  \n",
      "17   80.920721     7.614827  2.030097    448.786983   6.106548  \n",
      "18   71.857618    16.024618  2.774126    557.445086   6.323364  \n",
      "19   77.588036     6.675558  1.898453    232.221240   5.447691  \n",
      "20   31.792066     0.557386 -0.584497     32.850075   3.491954  \n",
      "21   79.406024     4.089394  1.408397    241.012284   5.484848  \n",
      "22   68.054838    38.573862  3.652575    773.328301   6.650704  \n",
      "23   73.142418     8.956442  2.192373    179.558628   5.190502  \n",
      "24   70.036267    32.633902  3.485352    654.244059   6.483480  \n",
      "25   65.109139    61.423932  4.117800   1231.426217   7.115928  \n",
      "26   43.367556    12.468978  2.523244    321.493812   5.772978  \n",
      "27   80.157228     5.698554  1.740212    335.849648   5.816664  \n",
      "28   74.530394    51.670360  3.944884   1035.886733   6.943013  \n",
      "29   38.548939     6.810652  1.918488    185.707918   5.224175  \n",
      "..         ...          ...       ...           ...        ...  \n",
      "514  60.937500   271.848601  5.605245   3505.684849   8.162141  \n",
      "515  70.541631    28.600499  3.353424    737.420778   6.603159  \n",
      "516  75.539692    15.240174  2.723935    530.156795   6.273173  \n",
      "517  79.236463     4.195602  1.434037    247.271744   5.510488  \n",
      "518  77.660436    11.925696  2.478695    307.486104   5.728430  \n",
      "519  77.299091     2.714621  0.998652    159.988717   5.075103  \n",
      "520  77.932910     4.548449  1.514786    117.274903   4.764521  \n",
      "521  76.339766    10.994353  2.397382    220.414655   5.395511  \n",
      "522  58.554863     1.867516  0.624609     64.964879   4.173847  \n",
      "523  73.669304     4.947380  1.598858    127.560739   4.848593  \n",
      "524  46.754392     0.419432 -0.868855     14.590681   2.680383  \n",
      "525  14.500355  1791.541368  7.490832  20717.200221   9.938720  \n",
      "526  35.416838    23.326381  3.149585    527.290780   6.267752  \n",
      "527  72.676249     3.314598  1.198336    115.304235   4.747574  \n",
      "528  75.949391     3.207814  1.165590    189.055533   5.242041  \n",
      "529   1.417323  4731.757532  8.462052  51207.609419  10.843643  \n",
      "530  78.441873     3.393704  1.221922    118.056086   4.771160  \n",
      "531  78.607559     8.217270  2.106238    211.869930   5.355973  \n",
      "532  78.543341    10.951849  2.393508    380.979691   5.942746  \n",
      "533  79.250790     3.912705  1.364229    230.598938   5.440680  \n",
      "534  34.934976    33.459835  3.510346    756.356614   6.628513  \n",
      "535  76.313723    10.450701  2.346669    269.455577   5.596404  \n",
      "536  39.030801    12.468978  2.523244    321.493812   5.772978  \n",
      "537  77.501176    26.812053  3.288852    537.527709   6.286980  \n",
      "538  35.657769    12.468978  2.523244    321.493812   5.772978  \n",
      "539  53.195349     3.098478  1.130911    107.786130   4.680149  \n",
      "540  77.252191    14.424753  2.668946    501.790882   6.218183  \n",
      "541  75.297893     3.537696  1.263476    208.497414   5.339927  \n",
      "542   4.192913   597.180456  6.392219   6905.733407   8.840107  \n",
      "543  78.450730    11.259214  2.421187    225.724584   5.419316  \n",
      "\n",
      "[544 rows x 30 columns]\n",
      "544 34 33\n"
     ]
    }
   ],
   "source": [
    "#Prepping Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "\n",
    "\n",
    "\n",
    "XLabels = ['TotalT','Temp','LSR','CA','Size','Mass','Moisture', 'IsoT', 'HeatT', 'Ramp','F_X', 'Ro', 'logRo', 'P']\n",
    "\n",
    "train = data.copy()\n",
    "test = data.copy()\n",
    "\n",
    "# counter = 0\n",
    "# for i in data.index:\n",
    "#     if not train.at[i, 'Source'] == \"Generation of xylose solutions from Eucalyptus globulus wood.xlsx\":\n",
    "#         counter = counter + 1\n",
    "\n",
    "# print(counter)\n",
    "\n",
    "# train = train[train['Source'] != \"Generation of xylose solutions from Eucalyptus globulus wood.xlsx\"]\n",
    "# test = test[test['Source'] == \"Generation of xylose solutions from Eucalyptus globulus wood.xlsx\"]\n",
    "\n",
    "train = train[train['Source'] != \"MildAutohydrolysisAnEnvironmentallyFriendlyTechnologXyl.xlsx\"]\n",
    "test = test[test['Source'] == \"MildAutohydrolysisAnEnvironmentallyFriendlyTechnologXyl.xlsx\"]\n",
    "\n",
    "\n",
    "\n",
    "train = train.sample(frac=1).reset_index(drop=True)\n",
    "test = test.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "X_train = train[['TotalT','Temp','LSR','CA','Size','Mass','Moisture', 'IsoT', 'HeatT', 'Ramp','F_X']].values\n",
    "y_train = train['Yield'].values\n",
    "\n",
    "# plt.hist(y_train)\n",
    "# plt.hist(y_test)\n",
    "\n",
    "X_test = test[['TotalT','Temp','LSR','CA','Size','Mass','Moisture', 'IsoT', 'HeatT', 'Ramp','F_X']].values\n",
    "y_test = test['Yield'].values\n",
    "\n",
    "print(train)\n",
    "\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=0)\n",
    "\n",
    "\n",
    "print(len(X_train), len(X_test), len(X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-09T03:51:31.779448Z",
     "start_time": "2019-02-09T03:51:31.200995Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average 53.27690074178741\n",
      "average 13.093757618710132\n",
      "Mean Absolute Error: 40.18314312307728\n",
      "Mean Squared Error: 1746.6978099844691\n",
      "Root Mean Squared Error: 41.79351397028574\n",
      "[51.51320088 61.65660915 50.49694777 52.03101345 48.87770809 70.17400919\n",
      " 48.53701209 78.69140922 73.5809692  51.65970016 52.10941888 50.190478\n",
      " 48.65057743 48.10782417 49.2184041  49.04805609 44.88500102 48.91286799\n",
      " 53.92646422 49.67266543 52.65468911 53.35863756 49.19678133 47.48533391\n",
      " 51.10452231 51.60514345 58.48720267 52.17787134 50.07107777 53.13920912\n",
      " 51.79711422 51.32613133 56.71558347 50.3549911 ]\n",
      "[ 0.97012048  7.25204819  2.70149398  0.83122892 11.40819277 24.42795181\n",
      "  1.81937349 39.01686747 29.64433735  1.83421687  5.46024096 14.72674699\n",
      "  4.10525301  0.         21.54409639 17.0486747   0.          1.01783133\n",
      " 30.58795181 35.07277108 31.21349398 22.95421687  3.14255422  0.\n",
      "  9.28771084  0.1060241  31.00144578 39.12289157  0.6106988   0.33079518\n",
      "  2.84144578 32.1253012  21.37445783  1.6073253 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 78.69140921830397)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Multiple Linear Regression Simple\n",
    "from sklearn import linear_model\n",
    "import statsmodels.api as sm\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "# with sklearn\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "coeffs = dict(zip(XLabels, regr.coef_))\n",
    "\n",
    "# print(\"results of SKLearnLinearRegression----------------\")\n",
    "# print('Intercept: \\n', regr.intercept_)\n",
    "# print('Coefficients: \\n', coeffs)\n",
    "# print('Score', regr.score(X,Y))\n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "print('average', y_pred.mean())\n",
    "\n",
    "print('average', y_test.mean())\n",
    "\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "#print(\"Coefficients: \", best_model.coef_)\n",
    "\n",
    "error_arr = abs(y_test - y_pred)\n",
    "#plt.hist(error_arr)\n",
    "\n",
    "print(y_pred)\n",
    "print(y_test)\n",
    "\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.ylim(np.min((np.min([np.min(y_test), np.min(y_pred)]))), np.max((np.max([np.max(y_test), np.max(y_pred)]))))\n",
    "plt.xlim(np.min((np.min([np.min(y_test), np.min(y_pred)]))), np.max((np.max([np.max(y_test), np.max(y_pred)]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-09T03:51:31.954978Z",
     "start_time": "2019-02-09T03:51:31.784444Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best alpha is:  0.001\n",
      "Lowest Error In Validation _MSE_  1826.3513078859467\n",
      "average 13.093757618710132\n",
      "Mean Absolute Error: 40.18333430012333\n",
      "Mean Squared Error: 1746.7132277006956\n",
      "Root Mean Squared Error: 41.79369842094255\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 78.69160207969847)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Linear Least Square with I2 regularization\n",
    "#Ridge\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "alphas = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 100]\n",
    "errors = []\n",
    "\n",
    "#Creating Different models for different alphas\n",
    "for a in alphas:\n",
    "    ridgeModel = Ridge(alpha=a)\n",
    "    ridgeModel.fit(X_train, y_train) \n",
    "    y_pred = ridgeModel.predict(X_valid)\n",
    "    error =  metrics.mean_squared_error(y_valid, y_pred)\n",
    "    errors.append(error)\n",
    "    \n",
    "\n",
    "best_alpha = alphas[np.argmin(errors)]\n",
    "print(\"best alpha is: \",best_alpha)\n",
    "print(\"Lowest Error In Validation _MSE_ \", np.min(errors))\n",
    "\n",
    "best_model = Ridge(alpha=best_alpha)\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "    \n",
    "#Evaluation\n",
    "print('average', y_test.mean())\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "#print(\"Coefficients: \", best_model.coef_)\n",
    "\n",
    "error_arr = abs(y_test - y_pred)\n",
    "#plt.hist(error_arr)\n",
    "plt.scatter(y_test, y_pred)\n",
    "\n",
    "plt.ylim(np.min((np.min([np.min(y_test), np.min(y_pred)]))), np.max((np.max([np.max(y_test), np.max(y_pred)]))))\n",
    "plt.xlim(np.min((np.min([np.min(y_test), np.min(y_pred)]))), np.max((np.max([np.max(y_test), np.max(y_pred)]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-09T03:51:36.895Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#SVM Models\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "kernels =  ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "gammas = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1]\n",
    "Cs =[0.1, 0.5, 1, 5, 10, 50, 100]\n",
    "#Cs =[0.1, 0.5, 1]\n",
    "errors = []\n",
    "\n",
    "for kern in kernels:\n",
    "    for gam in gammas:\n",
    "        for C_ in Cs:\n",
    "            svrModel = SVR(kernel=kern, gamma=gam, cache_size=1000, C=C_)\n",
    "            svrModel.fit(X_train, y_train) \n",
    "            y_pred = svrModel.predict(X_valid)\n",
    "            error =  metrics.mean_squared_error(y_valid, y_pred)\n",
    "            errors.append(error)\n",
    "        \n",
    "        \n",
    "#For 2D\n",
    "# index_of_lowest_error = np.argmin(errors)\n",
    "# print(\"Lowest Error In Validation _MSE_ \", np.min(errors))\n",
    "\n",
    "# best_kernel = kernels[int(index_of_lowest_error / len(gammas))]\n",
    "# best_gamma = gammas[index_of_lowest_error % len(gammas)]\n",
    "# print(\"Best kernel is: \", best_kernel)\n",
    "# print(\"Best Gamma is: \", best_gamma)\n",
    "\n",
    "#For3d\n",
    "index_of_lowest_error = np.argmin(errors)\n",
    "print(\"Lowest Error In Validation _MSE_ \", np.min(errors))\n",
    "\n",
    "best_kernel = kernels[int(index_of_lowest_error / (len(gammas) * len(Cs)))] #Good\n",
    "best_gamma = gammas[int((index_of_lowest_error % (len(gammas) * len(Cs)))/len(Cs))] #Good\n",
    "best_C = Cs[index_of_lowest_error % len(Cs)] #Good \n",
    "print(\"Best kernel is: \", best_kernel)\n",
    "print(\"Best Gamma is: \", best_gamma)\n",
    "print(\"Best C is: \", best_C)\n",
    "\n",
    "best_model = SVR(kernel=best_kernel, gamma=best_gamma, cache_size=1000, C=best_C)\n",
    "best_model.fit(X_train,y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "\n",
    "print('average', y_test.mean())\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "error_arr = abs(y_test - y_pred)\n",
    "#plt.hist(error_arr)\n",
    "\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.ylim(np.min((np.min([np.min(y_test), np.min(y_pred)]))), np.max((np.max([np.max(y_test), np.max(y_pred)]))))\n",
    "plt.xlim(np.min((np.min([np.min(y_test), np.min(y_pred)]))), np.max((np.max([np.max(y_test), np.max(y_pred)]))))\n",
    "# vectors = np.asarray(best_model.support_vectors_)\n",
    "# print(vectors.shape)\n",
    "# print(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average 0.5278839901926725\n",
      "Mean Absolute Error: 0.02878105149290653\n",
      "Mean Squared Error: 0.0018747801844422002\n",
      "Root Mean Squared Error: 0.043298731903396434\n"
     ]
    }
   ],
   "source": [
    "##Random Forest\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "import sklearn\n",
    "\n",
    "\n",
    "#Training\n",
    "regressor = RandomForestRegressor(n_estimators=20, random_state=0)  \n",
    "regressor.fit(X_train, y_train)  \n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#Evaluation\n",
    "print('average', y_test.mean())\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "\n",
    "#Getting Shape of Forest\n",
    "params = regressor.get_params()\n",
    "path = regressor.decision_path(X_train)\n",
    "\n",
    "#print(params)\n",
    "#print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "417 417\n",
      "90 90\n",
      "Epoch 1/40\n",
      "417/417 [==============================] - 0s 325us/step - loss: 0.2914 - acc: 0.0000e+00\n",
      "Epoch 2/40\n",
      "417/417 [==============================] - 0s 26us/step - loss: 0.2914 - acc: 0.0000e+00\n",
      "Epoch 3/40\n",
      "417/417 [==============================] - 0s 43us/step - loss: 0.2914 - acc: 0.0000e+00\n",
      "Epoch 4/40\n",
      "417/417 [==============================] - 0s 24us/step - loss: 0.2914 - acc: 0.0000e+00\n",
      "Epoch 5/40\n",
      "417/417 [==============================] - 0s 36us/step - loss: 0.2914 - acc: 0.0000e+00\n",
      "Epoch 6/40\n",
      "417/417 [==============================] - 0s 43us/step - loss: 0.2914 - acc: 0.0000e+00\n",
      "Epoch 7/40\n",
      "417/417 [==============================] - 0s 36us/step - loss: 0.2914 - acc: 0.0000e+00\n",
      "Epoch 8/40\n",
      "417/417 [==============================] - 0s 33us/step - loss: 0.2914 - acc: 0.0000e+00\n",
      "Epoch 9/40\n",
      "417/417 [==============================] - 0s 29us/step - loss: 0.2914 - acc: 0.0000e+00\n",
      "Epoch 10/40\n",
      "417/417 [==============================] - 0s 43us/step - loss: 0.2914 - acc: 0.0000e+00\n",
      "Epoch 11/40\n",
      "417/417 [==============================] - 0s 26us/step - loss: 0.2914 - acc: 0.0000e+00\n",
      "Epoch 12/40\n",
      "417/417 [==============================] - 0s 24us/step - loss: 0.2914 - acc: 0.0000e+00\n",
      "Epoch 13/40\n",
      "417/417 [==============================] - 0s 53us/step - loss: 0.2914 - acc: 0.0000e+00\n",
      "Epoch 14/40\n",
      "417/417 [==============================] - 0s 24us/step - loss: 0.2914 - acc: 0.0000e+00\n",
      "Epoch 15/40\n",
      "417/417 [==============================] - 0s 36us/step - loss: 0.2914 - acc: 0.0000e+00\n",
      "Epoch 16/40\n",
      "417/417 [==============================] - 0s 36us/step - loss: 0.2914 - acc: 0.0000e+00\n",
      "Epoch 17/40\n",
      "417/417 [==============================] - 0s 24us/step - loss: 0.2914 - acc: 0.0000e+00\n",
      "Epoch 18/40\n",
      "417/417 [==============================] - 0s 26us/step - loss: 0.2914 - acc: 0.0000e+00\n",
      "Epoch 19/40\n",
      "417/417 [==============================] - 0s 38us/step - loss: 0.2914 - acc: 0.0000e+00\n",
      "Epoch 20/40\n",
      "417/417 [==============================] - 0s 26us/step - loss: 0.2914 - acc: 0.0000e+00\n",
      "Epoch 21/40\n",
      "417/417 [==============================] - 0s 26us/step - loss: 0.2914 - acc: 0.0000e+00\n",
      "Epoch 22/40\n",
      "417/417 [==============================] - 0s 41us/step - loss: 0.2914 - acc: 0.0000e+00\n",
      "Epoch 23/40\n",
      "417/417 [==============================] - 0s 22us/step - loss: 0.2914 - acc: 0.0000e+00\n",
      "Epoch 24/40\n",
      "417/417 [==============================] - 0s 24us/step - loss: 0.2914 - acc: 0.0000e+00\n",
      "Epoch 25/40\n",
      "417/417 [==============================] - 0s 43us/step - loss: 0.2914 - acc: 0.0000e+00\n",
      "Epoch 26/40\n",
      "417/417 [==============================] - 0s 29us/step - loss: 0.2914 - acc: 0.0000e+00\n",
      "Epoch 27/40\n",
      "417/417 [==============================] - 0s 24us/step - loss: 0.2914 - acc: 0.0000e+00\n",
      "Epoch 28/40\n",
      "417/417 [==============================] - 0s 33us/step - loss: 0.2914 - acc: 0.0000e+00\n",
      "Epoch 29/40\n",
      "417/417 [==============================] - 0s 31us/step - loss: 0.2914 - acc: 0.0000e+00\n",
      "Epoch 30/40\n",
      "417/417 [==============================] - 0s 24us/step - loss: 0.2914 - acc: 0.0000e+00\n",
      "Epoch 31/40\n",
      "417/417 [==============================] - 0s 43us/step - loss: 0.2914 - acc: 0.0000e+00\n",
      "Epoch 32/40\n",
      "417/417 [==============================] - 0s 29us/step - loss: 0.2914 - acc: 0.0000e+00\n",
      "Epoch 33/40\n",
      "417/417 [==============================] - 0s 24us/step - loss: 0.2914 - acc: 0.0000e+00\n",
      "Epoch 34/40\n",
      "417/417 [==============================] - 0s 43us/step - loss: 0.2914 - acc: 0.0000e+00\n",
      "Epoch 35/40\n",
      "417/417 [==============================] - 0s 29us/step - loss: 0.2914 - acc: 0.0000e+00\n",
      "Epoch 36/40\n",
      "417/417 [==============================] - 0s 26us/step - loss: 0.2914 - acc: 0.0000e+00\n",
      "Epoch 37/40\n",
      "417/417 [==============================] - 0s 31us/step - loss: 0.2914 - acc: 0.0000e+00\n",
      "Epoch 38/40\n",
      "417/417 [==============================] - 0s 41us/step - loss: 0.2914 - acc: 0.0000e+00\n",
      "Epoch 39/40\n",
      "417/417 [==============================] - 0s 24us/step - loss: 0.2914 - acc: 0.0000e+00\n",
      "Epoch 40/40\n",
      "417/417 [==============================] - 0s 36us/step - loss: 0.2914 - acc: 0.0000e+00\n",
      "90/90 [==============================] - 0s 432us/step\n"
     ]
    }
   ],
   "source": [
    "#Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=4, activation='relu', input_dim=14))\n",
    "model.add(Dense(units=1, activation='softmax'))\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(len(X_train), len(y_train))\n",
    "print(len(X_test), len(y_test))\n",
    "\n",
    "model.fit(X_train, y_train, epochs=40, batch_size=32)\n",
    "\n",
    "loss_and_metrics = model.evaluate(X_test, y_test, batch_size=128)\n",
    "                                  \n",
    "classes = model.predict(X_test, batch_size=128)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
